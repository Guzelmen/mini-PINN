###############
# Config for Phase 4: Temperature-dependent Thomas-Fermi
# Inputs: [x, alpha, T_kV]
# SMALL RANGE: alpha ~1-5, T 1-10 keV
###############

base_config: &BASE_CONFIG
    ## parameters
  random_seed: 42

  run_name: "training_4thinp_logx_gpu_best_1e-6_model"
  wandb_notes: ""
## wandb parameters
  use_wandb: True  # Set False to disable all wandb logging
  wandb_group: "week23feb"
  wandb_project: "mini_pinn"
  wandb_entity: "guzelmen_msci_project"

  phase: 4 # temperature-dependent TF
  n_vars: 3  # network inputs
  debug_mode: False

  ## model parameters
  mode: "hard" # "soft" or "hard"
  inp_dim: 3  
  nlayers: 8
  hidden: 128
  activation: "SiLU" # SiLU, Tanh, ReLU, Identity, SIREN, GELU, Mish
  w0: 30
  add_phi0: False
  k_val: 10
  
  # phase 4 m-weighted loss: multiplies residual by x^(0.5+m) on LHS, x^(1.5+m) on RHS
  # m=-0.5 is equivalent to no m-weighting (baseline)
  use_m_loss: True
  m_loss_m_value: -0.5  # sweep values: -0.5, 0, 0.5, 1.0

  ## normalisation (alpha)
  norm_mode: "standardize"   # minmax or standardize
  standard_mean: 0.0  # Will be computed from train data
  standard_std: 1.0   # Will be computed from train data
  minmax_min: -1.0
  minmax_max: 1.0

  ## normalisation (T) - computed from train data
  T_mean: 0.0
  T_std: 1.0

  ## normalisation (x)
  use_log_x: True
  x_log_mean: 0.0   # Will be computed from train data
  x_log_std: 1.0    # Will be computed from train data

  ## optimizer parameters
  lr_type: "cosine_decaying_restarts" # constant, linear, cosine, cosine_restarts, cosine_decaying_restarts
  constant_lr: 1e-4
  start_lr: 1e-5
  end_lr: 1e-3
  lr_warmup_epochs: 200
  min_lr: 5e-5  # Minimum LR at the bottom of every cosine cycle
  cosine_restart_period: 800  # Epochs per cycle for cosine_restarts / cosine_decaying_restarts
  restart_decay_factor: 0.6   # Geometric decay of peak LR per cycle (cosine_decaying_restarts only)
  # cosine_decay_epochs: 2000  # Fixed cosine decay period (omit to use total_epochs - warmup)
  grad_clipping: True  # Clips to clip_value
  clip_value: 250.0

  ## loss parameters
  loss_type: "mse"
  # Residual normalization strategy for training
  # "coeff": divide by λ³/γ (current default)
  # "relative": divide by max(|LHS|, |RHS|) - per-sample relative error  
  # "rel_l2": divide by RHS L2 norm (over batch) - global relative L2 error"
  residual_norm_strategy: "rel_l2"
  
  fmt_help: False
  loss_strategy: "fixed" # "fixed" or "adaptive"
  fmt_weights: {"residual": 1.0, "fmt": 1.0}
  cancel_c: False

  ## hybrid training (data + physics loss)
  hybrid_training: True  # If True, combine PDE loss with data loss
  use_solver_data: True  # If True, load data in {inputs, targets} format
  data_loss_type: "rel_l2"  # options: "mse", "rel_l2"
  hybrid_weight_strategy: "fixed"  # "fixed" or "adaptive" (adaptive overrides schedule below)
  hybrid_weight_schedule: "cosine"  # "fixed", "step", or "cosine"
  # Starting weights (epoch 0)
  physics_loss_weight: 1.0
  data_loss_weight: 5.0
  deriv_loss_weight: 5.0
  # Final weights (last epoch) — cosine interpolation between start and end
  physics_loss_weight_final: 10.0
  data_loss_weight_final: 1.0
  deriv_loss_weight_final: 1.0
  data_loss_step_epochs: 500  # Only used when hybrid_weight_schedule: "step"

  ## derivative target loss
  use_deriv_loss: True        # If True, add dψ/dx matching loss from solver data

  ## data parameters
  data_path: "data/phase_4_solver_doubletargets_smallrange.pt"
  filter_x_min: True          # If True, remove data points with x < x_min_threshold before splitting
  x_min_threshold: 1e-6       # Minimum x value to keep (points below are removed)
  batch_size: 2048
  shuffle_train: True
  use_sqrtx: False

  ## training parameters
  stage: "train"
  epochs: 5000

  # saving
  save_preds: False
  save_every: 100
  plot_auto: False
  save_weights: True
  save_weights_every: 250
  pred_dir: "predictions"
  plot_dir: "plot_predictions"
